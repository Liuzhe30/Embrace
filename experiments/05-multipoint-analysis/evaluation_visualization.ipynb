{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Layer\n",
    "from tensorflow.keras.layers import Reshape, Concatenate\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.attention_weights = None  # Save attention weights here\n",
    "\n",
    "    def call(self, inputs, training, return_attention=False):\n",
    "        attn_output, attn_weights = self.att(inputs, inputs, return_attention_scores=True)\n",
    "        self.attention_weights = attn_weights  # Save attention weights\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        if return_attention:\n",
    "            return self.layernorm2(out1 + ffn_output), attn_weights\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "def build_cascade_transformer_model_with_fingerprint(input_shape, fingerprint_shape, num_heads=4, ff_dim=128, task1_classes=2, task2_classes=2):\n",
    "    # Primary input for sequence data\n",
    "    sequence_input = Input(shape=input_shape, name=\"sequence_input\")  # Shape: (sequence_length, feature_dim)\n",
    "\n",
    "    # Fingerprint input\n",
    "    fingerprint_input = Input(shape=(fingerprint_shape,), name=\"fingerprint_input\")  # Shape: (fingerprint_dim,)\n",
    "    fingerprint_dense = Dense(input_shape[1], activation=\"relu\", name=\"fingerprint_dense\")(fingerprint_input)  # Match feature_dim\n",
    "    fingerprint_expanded = Reshape((1, input_shape[1]), name=\"fingerprint_expanded\")(fingerprint_dense)  # Shape: (1, feature_dim)\n",
    "\n",
    "    # Combine sequence input and fingerprint input along the sequence dimension\n",
    "    combined_input = Concatenate(axis=1, name=\"combined_input\")([sequence_input, fingerprint_expanded])  # Shape: (sequence_length + 1, feature_dim)\n",
    "\n",
    "    # First Transformer Block\n",
    "    x = TransformerBlock(embed_dim=input_shape[1], num_heads=num_heads, ff_dim=ff_dim)(combined_input)\n",
    "    x = TransformerBlock(embed_dim=input_shape[1], num_heads=num_heads, ff_dim=ff_dim)(x)\n",
    "    x = Flatten()(x)\n",
    "    shared_dense = Dense(input_shape[1], activation='relu', name=\"shared_dense_1\")(x)\n",
    "    shared_dense_task1 = Dense(128, activation='relu', name=\"shared_dense_2\")(shared_dense)\n",
    "    shared_dense_task1 = Dense(32, activation='relu', name=\"shared_dense_3\")(shared_dense_task1)\n",
    "    shared_dropout = Dropout(0.3, name=\"shared_dropout_1\")(shared_dense_task1)\n",
    "\n",
    "    # Task 1 Output\n",
    "    task1_output = Dense(task1_classes, activation='softmax', name=\"task1_output\")(shared_dropout)\n",
    "\n",
    "    # Cascade Task 1 Output to Task 2 Input\n",
    "    task1_features = Dense(input_shape[1], activation='relu', name=\"task1_features\")(task1_output)\n",
    "    task1_features_expanded = tf.expand_dims(task1_features, axis=1)\n",
    "\n",
    "    # Expand shared_dense and concatenate with task1_features\n",
    "    shared_dense_expanded = tf.expand_dims(shared_dense, axis=1)\n",
    "    cascade_input = tf.concat([shared_dense_expanded, task1_features_expanded], axis=1)  # Shape: (batch_size, 2, input_shape[1])\n",
    "\n",
    "    # Task 2 Transformer Block\n",
    "    task2_x = TransformerBlock(embed_dim=input_shape[1], num_heads=num_heads, ff_dim=ff_dim)(cascade_input)\n",
    "    task2_x = Flatten()(task2_x)\n",
    "    task2_x = Dense(128, activation='relu', name=\"task2_dense_1\")(task2_x)\n",
    "    task2_x = Dense(32, activation='relu', name=\"task2_dense_2\")(task2_x)\n",
    "\n",
    "    # Task 2 Output\n",
    "    task2_output = Dense(task2_classes, activation='softmax', name=\"task2_output\")(task2_x)\n",
    "\n",
    "    # Build Model\n",
    "    model = Model(inputs=[sequence_input, fingerprint_input], outputs=[task1_output, task2_output], name=\"cascade_transformer_with_fingerprint\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.load('data/test_inputs.npy')\n",
    "test_fingerprints = np.load('data/test_fingerprints.npy')\n",
    "test_inputs = test_inputs.astype('float32')\n",
    "test_fingerprints = test_fingerprints.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = build_cascade_transformer_model_with_fingerprint(\n",
    "    input_shape=(1024, 80),\n",
    "    fingerprint_shape=881\n",
    ")\n",
    "model.load_weights('../../model/cascade_transformer_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[9.9287346e-30, 1.0000000e+00],\n",
      "       [1.8304142e-29, 1.0000000e+00],\n",
      "       [1.3748761e-29, 1.0000000e+00]], dtype=float32), array([[0.47746563, 0.5225344 ],\n",
      "       [0.47191617, 0.52808386],\n",
      "       [0.47424385, 0.5257562 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict([test_inputs, test_fingerprints])\n",
    "print(test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fff98fc3b3d81bd655c2cc48858186e4d9e2db7b515bf1c3221888f12a62f87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
